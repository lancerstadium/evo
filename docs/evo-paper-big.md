

## EVO Concept

AEP(*Autonomous Edge Learning and Inferencing Pipline*)

Memory Efficient Replacement Stratrgy

Binarized Neural Networks


### 1 Background 研究背景

EVO: A Memory Effective TinyML Inference Engine

近年来，边缘人工智能（Edge AI）发展迅速，由于其部署领域广、设备数量庞大、产生数据量多的特性备受关注。它是指在边缘设备上执行人工智能算法和模型的能力，旨在利用边缘计算的优势来处理和分析数据。边缘智能（Edge Intelligence, EI）是边缘人工智能的一个重要组成部分，边缘智能结合了边缘计算和人工智能，旨在通过在数据生成源附近进行数据处理和分析，提升响应速度和效率。它使得设备能够在本地进行智能决策，减少对云计算的依赖。
能够处理在边缘设备上生成的大量数据，利用本地计算能力进行实时分析。这种方法不仅提高了数据处理的速度，还降低了延迟，适合需要快速反应的应用场景。
应用领域：边缘智能在多个领域得到了广泛应用，包括智能家居、工业物联网（IIoT）、智能交通、医疗健康等。通过在这些领域中实施边缘智能，可以实现更高效的资源利用和更好的用户体验。
隐私与安全：由于数据在本地处理，边缘智能有助于提高数据隐私和安全性。敏感数据无需传输到云端，从而降低了数据泄露的风险。
挑战与机遇：尽管边缘智能具有许多优势，但在实现过程中仍面临挑战，如设备的计算能力限制、模型的优化和适应性问题等。然而，这些挑战也为研究和技术发展提供了新的机遇。
同时，深度神经网络（DNN）已成为各个领域的关键方法，包括图像分类、语音识别和自然语言处理等等，逐步向边缘设备上集成。
推理是指在应用程序中执行经过训练的 AI 模型。它在各种实时应用中变得越来越普遍，例如交互式服务、自动驾驶汽车和智能可穿戴设备。实时推理任务通常以小批量甚至一批量的形式出现。这些任务对延迟敏感，需要低延迟来确保用户体验或安全性。因此，实时 AI 推理对吞吐量和时延都有很高的要求。使用深度神经网络（DNN）的基本流程：模型定义、模型训练、模型推理。挑战：模型推理内存占用、推理速度、吞吐量等等。
解决方案：TinyML Engine 目的是赋予边缘侧设备计算能力，互联设备之间可以构成更大的系统，同时与云端交互。通过将计算任务下放到边缘侧... 
通过在边缘设备上执行机器学习推理，可以显著降低延迟和能耗。这种方法避免了将大量数据传输到云端进行处理，从而减轻了网络带宽的压力。

### 2 TinyML 推理引擎概览


#### 2.1 推理引擎架构


#### 2.2 常见的推理引擎

TinyML框架：
- TFLM
- MicroTVM
- CMSIS-NN
- TinyEngine
- ...


#### 2.3 推理优化

推理引擎指标：
1. 推理速度（Inference Speed）
2. 吞吐量（Throughput）
3. 结果质量（Results Quality）
4. 内存占用（Mem Usage）
5. 可拓展（Extensibility）
6. 易用性（）


##### 2.3.1 data-level 数据级优化

1. 输入压缩（Input Compression）
2. 输出组织（Output Organization）
3. 稀疏化

##### 2.3.2 model-level 模型级优化

高效结构设计（Efficient Structure Design）

模型压缩四件套：
1. 量化（Quantization）
2. 剪枝（Pruning）
3. 知识蒸馏（Knowledge Distillation）
4. 约束神经架构搜索（Constrained Neural Architecture Search）

##### 2.3.3 card-level (graph-level) 图优化

图拆分
跨核并行
数据布局
算子融合
批处理（提高内存重用）

##### 2.3.4 op-level 算子优化

内联汇编
硬件加速

##### 2.3.5 system-level 系统级优化

资源分配
加速器通信优化
提高CPU利用率和PCIe流量优化


#### 2.4 指标

GFLOPS


#### 2.5 挑战

TinyML的挑战：
- 内存占用（Memory Footprint）
- 处理器功耗（Processing Power）
- 内存带宽
- 大模型、复杂模型的推理


### 3 EVO 引擎框架概述


##### 3.1 Workloads 工作负载


#### 3.2 Architecture 引擎架构

EVO引擎架构：
- 模型压缩
- 模型优化
- 模型推理
- 硬件加速


### 4 数据级优化

#### 4.1 稀疏化

稀疏张量在深度学习中扮演着重要的角色，尤其是在处理大规模数据和模型时。以下是对稀疏张量的来源、使用和机会的简要探讨：

1. 稀疏张量的来源
稀疏张量是指在高维空间中大部分元素为零的张量。其来源主要包括：

数据特性：许多实际应用中的数据本质上是稀疏的，例如文本数据（词袋模型）、图像数据（大多数像素为背景）和社交网络数据（用户之间的连接稀疏）。
模型结构：在深度学习模型中，尤其是神经网络，某些层（如卷积层）可能会产生稀疏的激活值或权重。例如，使用剪枝技术后，模型中的许多权重可能会被置为零，从而形成稀疏结构。
2. 稀疏张量的使用
稀疏张量在深度学习中的使用主要体现在以下几个方面：

内存效率：稀疏张量可以显著减少存储需求，因为只存储非零元素及其索引。这对于在资源受限的设备上运行深度学习模型尤为重要。
计算效率：通过利用稀疏性，计算可以只针对非零元素进行，从而减少计算量，提高推理速度。例如，稀疏矩阵乘法可以显著加速深度学习模型的前向传播过程【T3】。
模型压缩：稀疏张量常用于模型压缩技术中，如权重剪枝和低秩分解，帮助减少模型的参数数量和计算复杂度【T5】。
3. 稀疏张量的机会
稀疏张量在深度学习中的应用提供了许多机会：

高效推理：随着边缘计算和物联网设备的普及，稀疏张量可以帮助在这些资源受限的环境中实现高效的推理，满足实时处理的需求【T1】。
新算法开发：针对稀疏张量的特性，研究人员可以开发新的算法和优化技术，以进一步提高模型的性能和效率。例如，稀疏注意力机制在处理长序列输入时表现出色【T6】。
硬件加速：许多现代硬件（如TPU和GPU）支持稀疏计算，可以利用稀疏张量的特性来提高计算效率。这为硬件设计和优化提供了新的方向

### 5 模型级优化

#### 5.1 Model NAS

边缘兼容的深度学习模型设计和神经架构搜索（Neural Architecture Search, NAS）是为了在资源受限的边缘设备上实现高效的深度学习推理而进行的研究和实践。
神经架构搜索（NAS）
自动化设计：NAS是一种自动化方法，用于搜索和优化神经网络架构。通过算法自动寻找最佳的网络结构，NAS可以减少手动设计的时间和复杂性。
搜索空间：NAS的关键在于定义一个合适的搜索空间，其中包含可能的网络架构。这个搜索空间可以包括不同的层类型、连接方式和超参数设置。
搜索策略：NAS可以采用多种搜索策略，如强化学习、进化算法和梯度下降等。通过这些策略，NAS能够在给定的搜索空间中找到最优或近似最优的网络架构。
计算开销：尽管NAS能够找到高效的网络架构，但其计算开销通常较大。为了解决这一问题，研究者们提出了多种方法，如使用代理模型来加速搜索过程，或采用差分架构搜索（DARTS）等新技术。

差分架构搜索（DARTS）是一种相对较新的神经架构搜索（NAS）方法，近年来在深度学习领域引起了广泛关注。DARTS通过使用梯度下降来优化网络架构，从而显著减少了搜索时间和计算资源的需求。以下是一些DARTS的前沿研究方向和进展：
DARTS-v2：在原始DARTS的基础上，DARTS-v2引入了更复杂的搜索空间和更有效的架构优化策略，进一步提高了模型的性能和搜索效率。
渐进式DARTS：这种方法通过逐步增加搜索空间的复杂性来优化架构，允许在较小的模型上进行初步搜索，然后逐步扩展到更复杂的模型。
应用实例：通过NAS技术，研究者们已经在边缘设备上实现了显著的性能提升。例如，使用NAS优化的模型在Raspberry Pi等边缘设备上实现了更快的推理速度和更小的模型尺寸。
硬件感知的DARTS
硬件感知架构搜索：研究者们正在探索将硬件特性（如计算能力、内存带宽等）纳入DARTS的搜索过程，以生成更适合特定硬件平台（如边缘设备、移动设备等）的模型。这种方法可以提高模型在实际应用中的推理效率【T2】

AutoML

边缘兼容的深度学习模型设计和神经架构搜索（NAS）是实现高效边缘推理的关键技术。通过这些方法，可以在资源受限的环境中有效地部署深度学习模型，满足实时性和性能的需求

##### 5.1.1 Binary Nerual Network 二进制网络支持

BNN（Binary Neural Network）
BNN 专用算子设计
QBConv2D: 
QQConv2D:
BBConv2D:
BBPointwiseConv2D:
BMaxPool2D:
BBFC:
BBQFC:

#### 5.2 Model Compress

压缩技术在使深度学习模型适应资源受限的边缘设备方面发挥着至关重要的作用。这些技术旨在减少模型的大小和计算需求，同时尽量保持模型的准确性。以下是一些主要的压缩技术及其应用：

1. 剪枝（Pruning）
剪枝技术通过去除神经网络中不重要的权重或神经元来减少模型的复杂性。常见的剪枝方法包括：

权重剪枝：根据权重的重要性（如绝对值）来去除小于某个阈值的权重。
结构剪枝：去除整个神经元或卷积核，而不仅仅是单个权重，从而减少计算量。
剪枝可以显著减少模型的参数数量和计算需求，适合在边缘设备上部署。

2. 量化（Quantization）
量化技术通过将模型中的浮点数权重转换为低精度表示（如8位整数）来减少模型的存储需求和计算复杂度。量化的主要方法包括：

权重量化：将权重从32位浮点数转换为8位或更低精度的整数。
激活量化：在推理过程中，将激活值也量化为低精度表示。
量化不仅减少了模型的存储需求，还可以利用低精度计算加速推理过程。

3. 知识蒸馏（Knowledge Distillation）
知识蒸馏是一种通过训练一个小型模型（学生模型）来模仿一个大型预训练模型（教师模型）的方法。学生模型通过学习教师模型的输出概率分布来获得知识，从而在保持较高准确性的同时减少模型的复杂性。

这种方法特别适合在边缘设备上部署，因为学生模型通常更小且计算效率更高。

4. 低秩分解（Low-Rank Decomposition）
低秩分解技术通过将高维权重矩阵分解为多个低秩矩阵的乘积来减少模型的参数数量。这种方法可以有效降低计算复杂度，同时保持模型的表达能力。

常见的低秩分解方法包括SVD（奇异值分解）和CP（CANDECOMP/PARAFAC）分解。

5. 结构化压缩
结构化压缩技术通过对网络结构进行优化来减少计算需求。例如，使用深度可分离卷积替代标准卷积，可以显著减少计算量和参数数量。这种方法在轻量级网络（如MobileNet）中得到了广泛应用。

6. 组合技术
在实际应用中，通常会结合多种压缩技术以获得最佳效果。例如，可以先进行剪枝，然后进行量化，最后应用知识蒸馏，以实现更高的压缩率和更好的性能。

结论：压缩技术为在资源受限的边缘设备上部署深度学习模型提供了有效的解决方案。通过这些技术，可以显著减少模型的大小和计算需求，同时保持可接受的准确性，从而实现高效的边缘推理


#### 5.3 Model Shared Weight

在权重共享中，相同的权重用于计算多个层或滤波器的输出，从而减少了唯一权重的数量[157]。在[157]中，权重共享是通过k均值聚类和霍夫曼编码来执行的。由于重复使用相同的权重，内存需求显着降低。哈希函数可用于对权重进行分组，然后使用每组中的一个权重[149]。通过使用矢量量化可以减少 CNN 中的参数数量[163]。矢量量化的概念是对一组参数使用一个有代表性的中心值。使用 K 均值聚类对参数进行分组，然后对每个聚类应用矢量量化，从而在精度下降 1% 的情况下减少 16–24 倍的参数。在深度压缩[157]中，不同权重的数量被分组，导致 AlexNet 中的卷积层和密集层分别只有 8 位和 4 位精度权重索引。权重获取分为两步：读取权重索引和基于权重索引读取共享权重。权重共享仅对节省内存有用，不会影响 MAC 或计算的精度。


#### 5.4 Model Skip Layer

如图 13 所示。在推理运行时，网络可以根据输入类型跳过一些层以节省计算量，而对准确性的影响可以忽略不计。将推理拆分为多个任务并以不断增加的复杂性顺序执行可以节省能源。例如，智能助手（例如 Google Home、Apple Siri 和 Apple Watch）中的语音识别使用两个网络。

网络结构跳过的思路（都需要 Profiler <= 时延预测、硬件感知、数据感知）：
- 跳过图像部分卷积：gather -> conv -> scatter
- 私有缓存命中：直接输出结果，主要用于 web 服务查询
- 设计模型路由，多出口：通过`bridge`连接到裁剪的子图上


### 6 近存推理

近存推理：将大部分在边缘节点进行推理，将整个网络及参数部署在设备中


1. 边缘计算

2. 分布式推理


分布式边缘推理是指在多个边缘设备或边缘服务器上并行进行深度学习模型的推理。其主要特点和优势包括：
降低延迟：通过在靠近数据源的边缘设备上进行推理，可以减少数据传输的延迟，提高响应速度。这对于实时应用（如视频监控、自动驾驶等）尤为重要【T5】。
负载均衡：分布式推理可以根据设备的负载情况动态分配推理任务，确保资源的高效利用。例如，当某个设备负载过高时，可以将推理请求转发到其他空闲设备上【T6】。
协同工作：在分布式推理中，多个设备可以协同工作，共同处理复杂的推理任务。例如，多个设备可以共享模型参数或中间结果，以提高推理的准确性和效率。
分布式推理的技术实现:
数据并行性：在分布式训练中，数据并行性是最常用的策略。每个设备处理不同的数据批次，并在每个训练周期结束时进行参数同步【T2】。
模型并行性：对于大型模型，可以将模型的不同部分分配到不同的设备上进行训练和推理。这种方法适用于模型参数过大，无法在单个设备上存储的情况【T2】。
协同推理框架：如Mobile Deep Inference (MODI)等框架，支持在边缘设备和边缘服务器之间动态分配推理任务，根据资源可用性选择合适的模型进行推理【T6】。


### 5 软硬件协同设计


拓展引擎对专用加速硬件的支持

硬件感知的神经架构搜索 (NAS)：通过考虑特定硬件平台的性能反馈，自动生成最优的神经网络架构。例如，ProxylessNAS能够为特定硬件找到合适的神经网络架构，从而提高推理效率


### 6 性能分析与可视化



### Reference 参考文献

1. [Efficient Acceleration of Deep Learning Inference on Resource-Constrained Edge Devices: A Review](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9985008)
2. [First-Generation Inference Accelerator Deployment at Facebook](https://arxiv.org/pdf/2107.04140)
3. [A Survey on Efficient Inference for Large Language Models](https://arxiv.org/pdf/2404.14294)
4. [HAQ: Hardware-Aware Automated Quantization with Mixed Precision](https://arxiv.org/pdf/1811.08886)